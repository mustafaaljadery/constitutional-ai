{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").to(\"mps\")\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sky blue is a color that is used to represent the sky. It is a color that is used to represent the sky. It is a color that is used to represent the sky. It is a color'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference Test\n",
    "text = \"Why is the sky blue?\"\n",
    "inputs = tokenizer.encode(text, return_tensors='pt').to(\"mps\")\n",
    "outputs = model.generate(inputs, max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text = generated_text.replace(text, '').strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RLAIF dataset\n",
    "constitution = \"Identify specific ways in which the assistantâ€™s last output code was incorrect, the code wasn't clear, or the output wasn't well written.\"\n",
    "constitution_revision = \"Please rewrite the assistant response making sure the output is correct, the code is very clear, and very well written.\"\n",
    "\n",
    "dataset = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "def add_to_csv(csv_name, prompt, answer):\n",
    "    pass\n",
    "\n",
    "def format_data(instruction, input, output):\n",
    "    user = f\"Instruction: {instruction} \\n Input: {input}\"\n",
    "    assistant = f\"Output: {output}\"\n",
    "\n",
    "    return {\n",
    "        \"user\": user, \n",
    "        \"assistant\": assistant\n",
    "    }\n",
    "\n",
    "def format_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        prompt += f\"{message['content']} \\n\"\n",
    "    return prompt\n",
    "\n",
    "for data in tqdm(dataset): \n",
    "    messages = []\n",
    "    formatted_data = format_data(data['instruction'], data['input'], data['output'])\n",
    "    messages.append({\"role\": \"user\", \"content\": formatted_data['user']})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": formatted_data['assistant']})\n",
    "    messages.append({\"role\": \"user\", \"content\": constitution})\n",
    "\n",
    "    prompt = format_prompt(messages)\n",
    "\n",
    "    initial_input = tokenizer.encode(prompt, return_tensors='pt').to(\"mps\")\n",
    "    initial_output = model.generate(initial_input, max_length=len(initial_input[0]) + 100, do_sample=True, temperature=0.4)\n",
    "    generated_text = tokenizer.decode(initial_output[0], skip_special_tokens=True)\n",
    "    generated_text = generated_text.replace(prompt, '').strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": generated_text})\n",
    "    messages.append({\"role\": \"user\", \"content\": constitution_revision})\n",
    "\n",
    "    prompt = format_prompt(messages)\n",
    "\n",
    "    final_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "    final_output = model.generate(final_input, max_length=len(final_output[0]) + 100, do_sample=True, temperature=0.4)\n",
    "    output_text = tokenizer.decode(final_output[0], skip_special_tokens=True)\n",
    "    output_text = output_text.replace(prompt, '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "\n",
    "# Dataset of python code\n",
    "dataset = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "# format prompt \n",
    "def format_prompt(prompt):\n",
    "    return prompt.replace(\"###\", \"\\n\")\n",
    "\n",
    "train_data = [(f\"Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n Instruction: {dataset[i]['instruction']} \\n Input: {dataset[i]['input']}\", f\"\\n Output: \\n {dataset[i]['output']}\") for i in range(len(dataset))]\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "train_encodings = tokenizer([f\"{q} {tokenizer.eos_token} {a}\" for q, a in train_data], truncation=True, padding=True)\n",
    "\n",
    "class DatasetClass(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "\n",
    "train_dataset = DatasetClass(train_encodings)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune Model \n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 3\n",
    "accumulation_steps = 4 \n",
    "device = \"mps\"\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_loader), start=1):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs[0]\n",
    "        loss = loss / accumulation_steps \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Free up memory\n",
    "        del input_ids, attention_mask, outputs, loss\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prompt = \"Write me a python function that adds 2 numbers together.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = model.generate(**inputs, max_length=125)\n",
    "outputs = tokenizer.decoe(outputs, skip_special_tokens=True)\n",
    "print(\"Generated responses:\", outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
